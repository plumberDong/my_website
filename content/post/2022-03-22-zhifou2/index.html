---
title: 用R对《知否知否》进行文本分析(二)
author: R package build
date: '2022-03-22'
slug: zhifou2
categories:
  - learning
tags:
  - Text Analysis
---



<pre class="r"><code>library(tidyverse)
library(tidytext)
library(chinese.misc)
library(jiebaR)
library(topicmodels) # lda算法
library(broom)
library(ggplot2)
library(showtext)
library(ggridges)
library(ggsci)
library(ggplot2)
showtext_auto()</code></pre>
<p>本文主要涉及两个知识点，一是由<code>tidy</code>生成<code>dtm</code>格式，二是进行<code>lda</code>建模。</p>
<div id="一从纯文本到tidy格式再到dtm" class="section level1">
<h1>一、从纯文本到tidy格式再到dtm</h1>
<p>首先，和<a href="https://plumberdong.netlify.app/2022/03/20/2022/">上一节</a>一样，我们生成一份tidy格式的《知否知否》数据。</p>
<pre class="r"><code>dat &lt;- scancn(&quot;知否知否.txt&quot;)
dat &lt;- str_split(dat, pattern = &quot;\\s&quot;)[[1]]

# 去除空行，生成tibble
dat &lt;- dat[dat!=&quot;&quot;] %&gt;%
  as_tibble()

# 生成章\回\句号
dat &lt;- dat %&gt;%
  rename(text = value) %&gt;%
  mutate(id1 = cumsum(str_detect(text,
                                     pattern = &quot;^第.+卷|^最终卷&quot;))) %&gt;%
  # 这里的回号不再于每一章开始时重置了
  mutate(id2 = cumsum(str_detect(text,
                                     pattern = &quot;^第.+回&quot;))) %&gt;%
  filter(id2 != 0) %&gt;%
  # 然后删除这些题目
  filter(!str_detect(text, patter = &quot;^第.+卷|^最终卷|^第\\d+回&quot;)) %&gt;%
  # 生成句子编号
  ungroup() %&gt;%
  mutate(id3 = row_number())

# 创建分词工具
mycutter &lt;- worker(user = &quot;知否_人名.txt&quot;) 

# 进行分词;打散；
dat &lt;- dat %&gt;%
  mutate(text = seg_file(text, from = &quot;v&quot;, mycutter = mycutter)) %&gt;%
  tidytext::unnest_tokens(word, text, pattern = &quot; &quot;, token = stringr::str_split)

# 剔除停止词
my_stop &lt;- read_lines(&quot;Chinese_StopWords.txt&quot;) %&gt;%
  as_tibble() %&gt;%
  rename(word = value)

dat1 &lt;- dat %&gt;%
  anti_join(my_stop, by = &quot;word&quot;) %&gt;%
  # 替换掉数字和英文字母
  mutate(word = gsub(pattern = &quot;\\d+|[A-Za-z]+&quot;,replacement = &quot;&quot;, x = word)) %&gt;%
  filter(word != &quot;&quot;)

rm(list = c(&quot;mycutter&quot;, &quot;dat&quot;, &quot;my_stop&quot;))

head(dat1)</code></pre>
<pre><code>## # A tibble: 6 x 4
##     id1   id2   id3 word 
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;
## 1     1     1     1 有人 
## 2     1     1     1 升官 
## 3     1     1     1 有人 
## 4     1     1     1 死   
## 5     1     1     1 翘   
## 6     1     1     1 穿越</code></pre>
<p>很多数据挖掘算法基于的格式是<code>dtm</code>。<code>dtm</code>是<em>Document-Term Matrix</em>的缩写。它本质上是一个大矩阵，行为对应节(document)，列为对应词项(term)，单元格内为对应词于对应节中出现的次数。</p>
<p>将<code>tidy</code>格式转化为<code>dtm</code>，需要用到<code>cast_dtm</code>函数。传递的三个参数为定义的节<code>document</code>，词项<code>term</code>以及词项出现的频次。</p>
<p>更多与tidy格式相关的转换方法，可以参见<a href="https://www.tidytextmining.com/dtm.html"><strong>Text Mining in R</strong></a>。</p>
<pre class="r"><code>my_dtm &lt;- dat1 %&gt;%
  group_by(id2) %&gt;%
  count(word) %&gt;%
  cast_dtm(id2, word, n) # cast_dtm</code></pre>
</div>
<div id="二主题建模" class="section level1">
<h1>二、主题建模</h1>
<p>生成<code>dtm</code>之后，就可以进行主题建模(<code>topic modeling</code>)了。主题模型的结构如图1所示：</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="lda_illustration.png" alt="主题模型图解" width="421" />
<p class="caption">
Figure 1: 主题模型图解
</p>
</div>
<p>我们假设在document和term（在本案例中为小节和词）之间存在一个topic层，topic的数量是一个超参数，需要主观判断选取。</p>
<p>有两组参数需要估计，<span class="math inline">\(\beta\)</span>代表了某一topic中对应term出现的概率，<span class="math inline">\(\gamma\)</span>则是某一document中有多少百分比的内容由对应topic生成。</p>
<p>对这些参数进行估计的方法主要是LDA（Latent Dirichlet allocation）。通过<code>topicmodels</code>中的<code>lda</code>函数可以调用该方法进行拟合，这里先设置两个主题<code>k=2</code>。</p>
<pre class="r"><code>my_lda &lt;- LDA(my_dtm, 
              k = 2, # 两个主题
              control = list(seed = 1234))</code></pre>
<div id="一-主题和词的关系" class="section level2">
<h2>(一) 主题和词的关系</h2>
<p>下面是<code>per-topic-per-word probabilities</code>，即每个topic中对应词出现的概率。</p>
<pre class="r"><code>my_topics &lt;- tidy(my_lda, matrix = &quot;beta&quot;)
my_topics %&gt;% head(10)</code></pre>
<pre><code>## # A tibble: 10 x 3
##    topic term      beta
##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;
##  1     1 爱    4.79e- 4
##  2     2 爱    2.96e- 4
##  3     1 安定  3.36e- 5
##  4     2 安定  1.06e- 7
##  5     1 安葬  1.12e- 5
##  6     2 安葬  8.42e-18
##  7     1 庵堂  6.73e- 5
##  8     2 庵堂  1.90e- 5
##  9     1 熬    1.43e- 4
## 10     2 熬    1.35e- 4</code></pre>
<p>下面绘制了各主题中最常出现的10个词：</p>
<pre class="r"><code>my_top_terms &lt;- my_topics %&gt;%
  group_by(topic) %&gt;%
  slice_max(beta, n = 10) %&gt;% 
  ungroup() %&gt;%
  arrange(topic, -beta)

my_top_terms %&gt;%
  mutate(term = reorder_within(term, beta, topic)) %&gt;%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  scale_y_reordered() + 
  labs(x = &quot;概率&quot;, y = NULL) + 
  scale_fill_aaas()</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" alt="两主题中分别出现概率最高的10个词" width="672" />
<p class="caption">
Figure 2: 两主题中分别出现概率最高的10个词
</p>
</div>
<p>这是两个主题中对应词的对数发生比：</p>
<pre class="r"><code>beta_wide &lt;- my_topics %&gt;%
  mutate(topic = paste0(&quot;topic&quot;, topic)) %&gt;%
  pivot_wider(names_from = topic, values_from = beta) %&gt;% 
  filter(topic1 &gt; .0023 | topic2 &gt; .0023) %&gt;% # 大于0.001概率出现
  mutate(log_ratio = log2(topic2 / topic1))

beta_wide %&gt;%
  ggplot(aes(x= reorder(term, log_ratio), y = log_ratio)) +
  geom_col() + 
  coord_flip() +
  labs(x = NULL)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" alt="两主题对应词的对数发生比" width="672" />
<p class="caption">
Figure 3: 两主题对应词的对数发生比
</p>
</div>
</div>
<div id="二小节和主题的关系" class="section level2">
<h2>（二）小节和主题的关系</h2>
<p>下面给出了对于各小节，分别有多少比例的词是由特定主题生成的。</p>
<pre class="r"><code>my_documents &lt;- tidy(my_lda, matrix = &quot;gamma&quot;)
my_documents %&gt;% head(10)</code></pre>
<pre><code>## # A tibble: 10 x 3
##    document topic     gamma
##    &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;
##  1 1            1 0.912    
##  2 2            1 1.00     
##  3 3            1 0.890    
##  4 4            1 1.00     
##  5 5            1 1.00     
##  6 6            1 0.583    
##  7 7            1 0.0000346
##  8 8            1 0.492    
##  9 9            1 0.0000578
## 10 10           1 0.0000609</code></pre>
<p>可以看下各小节由主题1生成的内容占比情况：</p>
<pre class="r"><code>my_documents %&gt;%
  filter(topic == 1) %&gt;%
  ggplot(aes(x = as.numeric(document), y= gamma)) + 
  geom_col()</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" alt="不同小节中由内容1生成的内容占比" width="672" />
<p class="caption">
Figure 4: 不同小节中由内容1生成的内容占比
</p>
</div>
</div>
<div id="三多主题" class="section level2">
<h2>（三）多主题</h2>
<p>之前我们只设置了2个主题。现在设置5个主题，看看基于这5个主题，是否能把五个章节区分开：</p>
<pre class="r"><code>my_lda_5 &lt;- LDA(my_dtm, 
              k = 5, # 两个主题
              control = list(seed = 1234))</code></pre>
<p>这是5个主题中出现概率最高的十个词：</p>
<pre class="r"><code>my_topics_5 &lt;- tidy(my_lda_5, matrix = &quot;beta&quot;)

my_top_terms_5 &lt;- my_topics_5 %&gt;%
  group_by(topic) %&gt;%
  slice_max(beta, n = 10) %&gt;% 
  ungroup() %&gt;%
  arrange(topic, -beta)

my_top_terms_5 %&gt;%
  mutate(term = reorder_within(term, beta, topic)) %&gt;%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  scale_y_reordered() + 
  labs(x = &quot;概率&quot;, y = NULL) + 
  scale_fill_aaas()</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" alt="5主题中分别出现概率最高的10个词" width="672" />
<p class="caption">
Figure 5: 5主题中分别出现概率最高的10个词
</p>
</div>
<p>我们再看一下不同章的小节中，属于不同主题的成分占比：</p>
<pre class="r"><code>my_documents_5 &lt;- tidy(my_lda_5, matrix = &quot;gamma&quot;)

# 章(id1)和小节(id2)的对应表
document_and_id1 &lt;- dat1 %&gt;%
  select(id1, id2) %&gt;%
  distinct(id1, id2)

my_documents_5 %&lt;&gt;% 
  mutate(document = as.integer(document)) %&gt;%
  # 找到不同小节的对应章
  left_join(document_and_id1, by = c(&quot;document&quot; = &quot;id2&quot;))

head(my_documents_5)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   document topic gamma   id1
##      &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;
## 1        1     1 1.00      1
## 2        2     1 0.218     1
## 3        3     1 1.00      1
## 4        4     1 1.00      1
## 5        5     1 1.00      1
## 6        6     1 1.00      1</code></pre>
<p>下图是不同章中，不同topic在小节中所占比例的概率密度分布。可以看到，区分效果并不是特别的好。从某种意义上说，这意味着五个章节并不是分属五个区别明显的主题。</p>
<pre class="r"><code>my_documents_5 %&gt;%
  ggplot(aes(x = gamma, y = factor(topic), fill = factor(topic))) + 
  geom_density_ridges(alpha = .8, color = &quot;white&quot;) + 
  facet_wrap(~id1) + 
  theme_ridges() + 
  theme(legend.position = &quot;none&quot;) + 
  scale_fill_futurama() + 
  labs(y = &quot;核密度&quot;, x = &quot;所占比例&quot;)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-14"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" alt="不同章节中不同topic在各小节所占比例的分布情况" width="672" />
<p class="caption">
Figure 6: 不同章节中不同topic在各小节所占比例的分布情况
</p>
</div>
</div>
</div>
